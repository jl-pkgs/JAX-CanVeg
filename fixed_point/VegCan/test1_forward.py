import jax
import jax.numpy as jnp
from jax import grad, jvp
from typing import List
from forward_eqx import Para, fixed_point, fixed_point_forward
from case import *


# ============================================================================
# 5. æµ‹è¯•å‡½æ•°
# ============================================================================
def test_forward():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("=" * 80)
    print("æµ‹è¯• 1: å‰å‘ä¼ æ’­")
    print("=" * 80)
    
    batch_size = 4
    dim = 3
    niter = 50
    
    key = jax.random.PRNGKey(42)
    bias = jax.random.normal(key, (batch_size, dim))
    
    # ä½¿ç”¨ eqx.Module åˆ›å»ºå‚æ•°
    para = Para(
        weight=jnp.ones((batch_size, dim)),
        bias=bias
    )
    
    states_guess = [jnp.zeros((batch_size, dim))]
    
    result = fixed_point_forward(
        states_guess,
        para,
        [],
        iter_func=simple_iter_func,
        update_substates_func=update_substates_func,
        get_substates_func=get_substates_func,
        niter=niter,
    )
    
    expected = 2.0 * bias
    
    print(f"æ•°å€¼è§£:\n{result}")
    print(f"\nè§£æè§£:\n{expected}")
    print(f"\næœ€å¤§è¯¯å·®: {jnp.max(jnp.abs(result - expected)):.6e}")
    
    assert jnp.allclose(result, expected, atol=1e-5), "å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥ï¼"
    print("\nâœ… å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡ï¼\n")


def test_jvp():
    """æµ‹è¯• JVP"""
    print("=" * 80)
    print("æµ‹è¯• 2: JVP (å‰å‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†)")
    print("=" * 80)
    
    batch_size = 2
    dim = 2
    niter = 50
    
    key = jax.random.PRNGKey(123)
    bias = jax.random.normal(key, (batch_size, dim))
    para = Para(
        weight=jnp.ones((batch_size, dim)),
        bias=bias
    )
    
    states_guess = [jnp.zeros((batch_size, dim))]
    
    def f(bias_param):
        para_temp = Para(weight=para.weight, bias=bias_param)
        return fixed_point_forward(
            states_guess,
            para_temp,
            [],
            iter_func=simple_iter_func,
            update_substates_func=update_substates_func,
            get_substates_func=get_substates_func,
            niter=niter,
        )
    
    v = jnp.ones_like(bias)
    result, tangent = jvp(f, (bias,), (v,))
    expected_tangent = 2.0 * v
    
    print(f"æ•°å€¼ JVP:\n{tangent}")
    print(f"\nè§£æ JVP:\n{expected_tangent}")
    print(f"\næœ€å¤§è¯¯å·®: {jnp.max(jnp.abs(tangent - expected_tangent)):.6e}")
    
    assert jnp.allclose(tangent, expected_tangent, atol=1e-4), "JVP æµ‹è¯•å¤±è´¥ï¼"
    print("\nâœ… JVP æµ‹è¯•é€šè¿‡ï¼\n")


def test_grad():
    """æµ‹è¯•æ¢¯åº¦è®¡ç®—"""
    print("=" * 80)
    print("æµ‹è¯• 3: æ¢¯åº¦è®¡ç®— (åå‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†)")
    print("=" * 80)
    
    batch_size = 2
    dim = 2
    niter = 50
    
    key = jax.random.PRNGKey(456)
    bias = jax.random.normal(key, (batch_size, dim))
    para = Para(
        weight=jnp.ones((batch_size, dim)),
        bias=bias
    )
    
    states_guess = [jnp.zeros((batch_size, dim))]
    
    def loss_fn(bias_param):
        para_temp = Para(weight=para.weight, bias=bias_param)
        result = fixed_point_forward(
            states_guess,
            para_temp,
            [],
            iter_func=simple_iter_func,
            update_substates_func=update_substates_func,
            get_substates_func=get_substates_func,
            niter=niter,
        )
        return jnp.sum(result ** 2)
    
    grad_fn = grad(loss_fn)
    numerical_grad = grad_fn(bias)
    analytical_grad = 8.0 * bias
    
    print(f"æ•°å€¼æ¢¯åº¦:\n{numerical_grad}")
    print(f"\nè§£ææ¢¯åº¦:\n{analytical_grad}")
    print(f"\næœ€å¤§è¯¯å·®: {jnp.max(jnp.abs(numerical_grad - analytical_grad)):.6e}")
    
    assert jnp.allclose(numerical_grad, analytical_grad, atol=1e-3), "æ¢¯åº¦æµ‹è¯•å¤±è´¥ï¼"
    print("\nâœ… æ¢¯åº¦æµ‹è¯•é€šè¿‡ï¼\n")


# ============================================================================
# 6. è¿è¡Œæ‰€æœ‰æµ‹è¯•
# ============================================================================
if __name__ == "__main__":
    print("\n" + "ğŸ”¥" * 40)
    print("å¼€å§‹æµ‹è¯• implicit_func_fixed_point (ä¿®å¤ç‰ˆ)")
    print("ğŸ”¥" * 40 + "\n")
    
    try:
        test_forward()
        test_jvp()
        test_grad()
        
        print("=" * 80)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 80)
        
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"\nğŸ’¥ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise
